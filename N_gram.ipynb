{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD9ZRNeIJ_Br"
      },
      "source": [
        "# Download and Process the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olLNujvMbg8",
        "outputId": "f8eedc09-6d80-43c5-85d9-f338f9f878ad"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet gdown\n",
        "\n",
        "# # 1. Download the zipped IMDB dataset from Drive\n",
        "# # this is the unsup part of https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "# !gdown \"https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\" -O imdb_dataset.zip\n",
        "\n",
        "# # 2. Unzip the downloaded file\n",
        "# !unzip -q imdb_dataset.zip -d imdb_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "siDkX864cazJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from math import log, exp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G1K-8ybJJ3Cl"
      },
      "outputs": [],
      "source": [
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    Splits text by newline, strips text, and returns a list of raw lines.\n",
        "    Replaces <br /> tags with a special token <nl>.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "            content = content.replace(\"<br />\", \" <nl> \")  # Ensure <nl> is spaced correctly\n",
        "            sentences = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
        "            all_sentences.extend(sentences)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    text = text.replace(\"<nl>\", \" <nl> \")  # Temporarily isolate <nl>\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = text.replace(\" <nl> \", \" <nl> \")  # Restore <nl> token placement\n",
        "    return text\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "    vocab = set()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower().strip() \n",
        "        sentence = remove_punctuation(sentence)\n",
        "        tokens = sentence.split() \n",
        "        vocab.update(tokens)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def tokinize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = remove_punctuation(sentence)\n",
        "        tokens = [token if token in vocab else unknown for token in sentence.split()]\n",
        "        tokenized_sentences.append(tokens)\n",
        "\n",
        "    return tokenized_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5469mMKcRP",
        "outputId": "fba195bf-23c9-4f9d-a383-24dfe067779a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of raw sentences loaded: 50000\n",
            "Example (first 2 sentences):\n",
            "[\"Sitting down for Macbeth the Comedy I was rather expecting a kind of Iris Murdochesque production, full of in-jokes about Shakespeare and Macbeth in particular. How wrong I was. Macbeth: The Comedy takes Shakespeare's well-crafted tale and adds a liberal sprinkling of modern low-level American humour. Apart from lots and lots and lots (many) jokes about gays and lesbians there was an attempt to lace the movie with slapstick comedy, presumably lightening the scene. This didn't work at all, being more the kind of slaphead comedy that doesn't make it much onto tv any more. I was pleasantly surprised to find no toilet humour, but the tone wasn't held much about that with much sniggering concerning the abovementioned lesbian and gay characters in the film: for which one must assume there is a reason. Somewhere.\", \"My problem with this film is that it tries to do too much, and doesn't have the time to adequately develop any of its themes to a satisfying conclusion. Romantic comedy, screwball comedy, political intrigue, comedy of manners\\x97it has so many angles that it leaves one rather unsatisfied. Monroe, even with her sometimes doe-eyed stupor, is still captivating---I am a fan, and part of her appeal to me is her sometimes obvious lack of polished acting tricks. She sometimes walks a tightrope in her performance, just on the edge of tumbling into abject amateurism, and that tension is definitely part of her appeal. You can't take her eyes off of her\\x85a bit like watching the Indy 500 and secretly dreading yet anticipating a fatal crash, all the while captivated by the sheer beauty of the event. Laurence Olivier, on the other hand, is imminently forgettable in this\\x85choosing to play the character as a stereotype. We learn so little about what makes him tick, other than the obvious fact that he keeps his emotions bottled-up. And there is something reptilian about his eyes which keep me from ever regarding him as someone's romantic interest. They always looked narrow and conniving, and it just got worse with age. Yes, there seemed little chemistry between them, and in spite of what I just said about his eyes, there just wasn't any time devoted to letting that chemistry develop. Did I miss something, or did Marilyn announce her love for him in the midst of his predictable, bumbling attempts to be seductive? If so, I must assume she was just overwhelmed by his looks, and I find this incomprehensible. Overall, this remains a weak entry in Marilyn's oeuvre. It was visually delightful, with a sort of premeditated artifice which lent it a storybook charm. If only the writing could have accomplished the same.\"]\n"
          ]
        }
      ],
      "source": [
        "imdb_folder = \"imdb_data/unsup\"\n",
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "print(f\"Number of raw sentences loaded: {len(sentences)}\")\n",
        "print(f\"Example (first 2 sentences):\\n{sentences[:2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qv0J6dGhIidP"
      },
      "outputs": [],
      "source": [
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ1sWIUG7rci"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9hA3B8WEKAF_"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "    Shuffle the sentences and split them into train and test sets.\n",
        "    First (1-test_split) of the data is used for training, the rest for testing.\n",
        "\n",
        "    Args:\n",
        "        sentences: List of sentences to split\n",
        "        test_split: Proportion of data to use for testing (default: 0.1)\n",
        "\n",
        "    Returns:\n",
        "        train_sentences, test_sentences: The split datasets\n",
        "    \"\"\"\n",
        "    # Create a copy of the sentences to avoid modifying the original\n",
        "    shuffled_sentences = sentences.copy()\n",
        "\n",
        "    # Shuffle the sentences (with the fixed seed for reproducibility)\n",
        "    random.shuffle(shuffled_sentences)\n",
        "\n",
        "    # Calculate the split point\n",
        "    split_idx = int(len(shuffled_sentences) * (1 - test_split))\n",
        "\n",
        "    # Split the data\n",
        "    train_sentences = shuffled_sentences[:split_idx]\n",
        "    test_sentences = shuffled_sentences[split_idx:]\n",
        "\n",
        "    return train_sentences, test_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfYWg45aKNzP",
        "outputId": "b790f357-beb4-4510-b194-f15335cf179b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 45000\n",
            "Number of test sentences: 5000\n"
          ]
        }
      ],
      "source": [
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of test sentences: {len(test_sentences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i9Hh9ptkKS6Y"
      },
      "outputs": [],
      "source": [
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_q9qARKw_u",
        "outputId": "2f839654-7358-4bb8-cfef-7db302790fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 161322\n",
            "Example tokens from first sentence: ['i', 'just', 'came', 'back', 'home', 'from', 'seeing', 'this', 'movie', 'and'] ...\n"
          ]
        }
      ],
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokenized_sentences = tokinize(train_sentences, vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example tokens from first sentence: {tokenized_sentences[0][:10] if tokenized_sentences else 'No tokens loaded'} ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9lbynIF5K6xJ"
      },
      "outputs": [],
      "source": [
        "# assert len(vocab) == 161573, \"Expected a vocabulary size of 161,573.\"\n",
        "assert len(tokenized_sentences) == 45000, \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert len(example) == 70, \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokinize([example], vocab)[0]\n",
        "assert len(example_tokens) == 13, \"Token count for the example sentence does not match the expected 13.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9bQ5K2ubNFhD"
      },
      "outputs": [],
      "source": [
        "def pad_sentence(tokens, n):\n",
        "    \"\"\"\n",
        "    Pads a list of tokens with <s> at the start (n-1 times)\n",
        "    and </s> at the end (once).\n",
        "    For example, if n=3, you add 2 <s> tokens at the start.\n",
        "    \"\"\"\n",
        "    return [\"<s>\"] * (n - 1) + tokens + [\"</s>\"]\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    Each sentence is padded with <s> and </s>.\n",
        "\n",
        "    Args:\n",
        "        tokenized_sentences: list of lists, where each sub-list is a tokenized sentence.\n",
        "        n: the order of the n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "        ngram_counts: Counter of n-grams (tuples of length n).\n",
        "        context_counts: Counter of (n-1)-gram contexts.\n",
        "    \"\"\"\n",
        "\n",
        "    #@n=2 --> P(baseball|Love) = C(love baseball)/C(love)\n",
        "    \n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "\n",
        "        for i in range(len(padded_sentence) - (n - 1)):\n",
        "            ngram = tuple(padded_sentence[i : i + n])\n",
        "            context = tuple(padded_sentence[i : i + n - 1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the probability of an n-gram using Laplace (add-alpha) smoothing.\n",
        "\n",
        "    P(w_i | w_{i-(n-1)}, ..., w_{i-1}) =\n",
        "        (count(ngram) + alpha) / (count(context) + alpha * vocab_size)\n",
        "\n",
        "    Args:\n",
        "        ngram: tuple of tokens representing the n-gram\n",
        "        ngram_counts: Counter of n-grams\n",
        "        context_counts: Counter of (n-1)-gram contexts\n",
        "        vocab_size: size of the vocabulary\n",
        "        alpha: smoothing parameter (1.0 = add-1 smoothing)\n",
        "\n",
        "    Returns:\n",
        "        Probability of the given n-gram.\n",
        "    \"\"\"\n",
        "    count_ngram = ngram_counts.get(ngram,0)\n",
        "    count_context = context_counts.get(ngram[:-1],0)\n",
        "\n",
        "    prob = (count_ngram + alpha) / (count_context + alpha * vocab_size)\n",
        "    return prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFRligyRx_8",
        "outputId": "6c424584-9990-429c-c986-e25fe341d2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of bigrams: 2281144\n",
            "Number of contexts: 161323\n"
          ]
        }
      ],
      "source": [
        "n = 2\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "161322"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v4NzCsBMzTN",
        "outputId": "4df8eaad-9fd9-43d9-ed24-9408668d9f83"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(context_tokens, ngram_counts, context_counts, vocab, n=2, alpha=1.0, top_k=5):\n",
        "    \"\"\"\n",
        "    Given a list of context tokens, predict the next token using the n-gram model.\n",
        "    Returns the top_k predictions as (token, probability).\n",
        "    \"\"\"\n",
        "    context = tuple(context_tokens[-(n-1):])  # Extract the last (n-1) tokens as context\n",
        "    candidates = []\n",
        "\n",
        "    for word in vocab:\n",
        "        ngram = context + (word,)\n",
        "\n",
        "        prob = laplace_probability(ngram,ngram_counts,context_counts,len(vocab),alpha)\n",
        "        candidates.append((word, prob))\n",
        "\n",
        "    # Sort candidates by probability in descending order and return the top_k\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def generate_text_with_limit(start_tokens, ngram_counts, context_counts, vocab, n=2, alpha=1.0, max_length=20):\n",
        "    \"\"\"\n",
        "    Generates text from an n-gram model until it sees </s>\n",
        "    or reaches a maximum total length (max_length).\n",
        "\n",
        "    Args:\n",
        "      start_tokens (list): initial context to begin generation\n",
        "      ngram_counts (Counter): trained n-gram counts\n",
        "      context_counts (Counter): trained (n-1)-gram counts\n",
        "      vocab (set): the model vocabulary\n",
        "      n (int): n-gram order, 2 for bigram, 3 for trigram, etc.\n",
        "      alpha (float): Laplace smoothing parameter\n",
        "      max_length (int): maximum number of tokens to generate (including start_tokens)\n",
        "\n",
        "    Returns:\n",
        "      A list of tokens representing the generated sequence.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    generated = list(start_tokens)\n",
        "    generated = list(start_tokens)\n",
        "    generator = torch.Generator(device='cpu')\n",
        "    generator.manual_seed(1100)\n",
        "\n",
        "    for _ in range(max_length - len(start_tokens)):\n",
        "        top_predictions = predict_next_token(generated, ngram_counts, context_counts, vocab, n, alpha, top_k=4)\n",
        "\n",
        "        if not top_predictions:  # Stop if no predictions are found\n",
        "            break\n",
        "        \n",
        "        # next_word = top_predictions[0][0]\n",
        "        words, probs = zip(*top_predictions)\n",
        "        probs_tensor = torch.tensor(probs, dtype=torch.float32)\n",
        "        probs_tensor /= probs_tensor.sum() \n",
        "        next_word_idx = torch.multinomial(probs_tensor, num_samples=1, generator=generator,replacement=True).item()\n",
        "        next_word = words[next_word_idx]\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if next_word == \"</s>\":  # Stop when end token is reached\n",
        "            break\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of 2-grams: 2281144\n",
            "Number of 3-grams: 6111347\n",
            "Number of 4-grams: 8809559\n"
          ]
        }
      ],
      "source": [
        "ngram_counts_dict = {}\n",
        "context_counts_dict = {}\n",
        "for n in [2,3,4]:\n",
        "    ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "    ngram_counts_dict[n] = ngram_counts\n",
        "    context_counts_dict[n] = context_counts\n",
        "    print(f\"Number of {n}-grams: {len(ngram_counts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Sequence at n=2:  ['i', 'love', 'and', 'the', 'film', 'and', 'i', 'dont', 'know', 'the', 'movie', 'is', 'the', 'story', 'of', 'the', 'film', 'is', 'a', 'good', 'but', 'i', 'was', 'a', 'good', 'as', 'a', 'lot', 'of', 'the', 'film', 'and', 'the', 'film', 'is', 'the', 'first', 'movie', 'and', 'the', 'film', 'and', 'the', 'film', 'is', 'not', 'a', 'great', 'movie', 'is', 'a', 'good', 'and', 'the', 'movie', 'is', 'a', 'movie', 'and', 'i', 'dont', 'know', 'that', 'i']\n",
            "Generated Sequence at n=3:  ['i', 'love', 'the', 'movie', 'is', 'the', 'best', 'movie', 'ever', 'made', 'it', 'to', 'the', 'end', 'of', 'the', 'film', 'is', 'a', 'great', 'deal', 'of', 'the', 'film', 'was', 'a', 'great', 'movie', 'to', 'be', 'a', 'little', 'bit', 'of', 'a', 'film', 'with', 'some', 'very', 'funny', 'and', 'entertaining', 'as', 'the', 'film', 'and', 'the', 'acting', 'is', 'terrible', 'and', 'i', 'was', 'very', 'good', 'and', 'the', 'movie', 'and', 'the', 'acting', 'is', 'very', 'much']\n",
            "Generated Sequence at n=4:  ['i', 'love', 'lizzie', 'received', 'mattered', 'received', 'received', 'revene', 'mattered', 'lizzie', 'lizzie', 'lizzie', 'received', 'lizzie', 'received', 'received', 'mattered', 'received', 'mattered', 'received', 'revene', 'received', 'revene', 'revene', 'received', 'received', 'mattered', 'mattered', 'lizzie', 'lizzie', 'mattered', 'received', 'revene', 'mattered', 'mattered', 'lizzie', 'revene', 'revene', 'lizzie', 'mattered', 'mattered', 'received', 'revene', 'lizzie', 'mattered', 'lizzie', 'revene', 'revene', 'mattered', 'mattered', 'revene', 'received', 'mattered', 'received', 'received', 'mattered', 'mattered', 'lizzie', 'lizzie', 'received', 'revene', 'lizzie', 'revene', 'lizzie']\n"
          ]
        }
      ],
      "source": [
        "for n in [2,3,4]:\n",
        "    context = [\"i\", \"love\"]\n",
        "    generated_seq = generate_text_with_limit(\n",
        "        start_tokens=context,\n",
        "        ngram_counts=ngram_counts_dict[n],\n",
        "        context_counts=context_counts_dict[n],\n",
        "        vocab=vocab,\n",
        "        n=n,\n",
        "        alpha=1.0,\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "    print(f\"Generated Sequence at n={n}: \", generated_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8tkY1BmAFxmG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(tokenized_sentences, ngram_counts, context_counts, vocab_size, n=2, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of an n-gram model (with Laplace smoothing)\n",
        "    on a list of tokenized sentences.\n",
        "\n",
        "    Args:\n",
        "      tokenized_sentences: List of lists of tokens.\n",
        "      ngram_counts: Counter of n-grams.\n",
        "      context_counts: Counter of (n-1)-grams.\n",
        "      vocab_size: Size of the vocabulary.\n",
        "      n: n-gram order.\n",
        "      alpha: Laplace smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "      A float representing the perplexity on the given dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    log_prob_sum = 0.0 \n",
        "    word_count = 0 \n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence = pad_sentence(sentence,n)\n",
        "\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i : i + n])  \n",
        "            prob = laplace_probability(ngram,ngram_counts,context_counts,vocab_size,alpha)\n",
        "\n",
        "            log_prob_sum += math.log(prob) \n",
        "            word_count += 1\n",
        "            # print(f\"N-gram: {ngram}, Probability: {prob}\")  # Debugging\n",
        "\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / word_count)  # Compute perplexity\n",
        "    return perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ExBYFpgj30"
      },
      "source": [
        "# **Analysis**\n",
        "use different n and rerun the code and write down your analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity at n=2: 577.3235912581922\n",
            "Perplexity at n=3: 3546.183560479877\n",
            "Perplexity at n=4: 27680.87531632101\n"
          ]
        }
      ],
      "source": [
        "tokenized_test_sentences = tokinize(test_sentences, vocab)\n",
        "for n in [2,3,4]:\n",
        "    ngram_counts = ngram_counts_dict[n]\n",
        "    context_counts = context_counts_dict[n]\n",
        "    pp = calculate_perplexity(tokenized_test_sentences, ngram_counts, context_counts, len(vocab), n, alpha=0.001)\n",
        "    print(f\"Perplexity at n={n}: {pp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity at n=2: 186.52792147362118\n",
            "Perplexity at n=3: 110.82426811321623\n",
            "Perplexity at n=4: 126.74022023698798\n"
          ]
        }
      ],
      "source": [
        "tokenized_train_sentences = tokinize(train_sentences[:200], vocab)\n",
        "for n in [2,3,4]:\n",
        "    ngram_counts = ngram_counts_dict[n]\n",
        "    context_counts = context_counts_dict[n]\n",
        "    pp = calculate_perplexity(tokenized_train_sentences, ngram_counts, context_counts, len(vocab), n, alpha=0.001)\n",
        "    print(f\"Perplexity at n={n}: {pp}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
